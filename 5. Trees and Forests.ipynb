{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v tibble  2.1.1       v purrr   0.3.2  \n",
      "v tidyr   0.8.3       v dplyr   0.8.0.1\n",
      "v readr   1.3.1       v stringr 1.4.0  \n",
      "v tibble  2.1.1       v forcats 0.4.0  \n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "suppressWarnings(library(rpart, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(rpart.plot, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(ggplot2, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(tidyverse, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(ranger, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(e1071, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(caret, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(randomForest, quietly = TRUE, warn.conflicts = FALSE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this module you will learn how to:\n",
    "\n",
    "1. Build a nonparametric model of house prices using regression trees\n",
    "2. Examine random forests and what the hyperparameters correspond to\n",
    "\n",
    "\n",
    "### The Data\n",
    "\n",
    "In this module we will re-examine the Housing data. However, instead of assuming the model is linear ($Y = X\\beta + \\varepsilon$) we will be agnostic about functional form, i.e. $Y=g(X)+\\varepsilon$. To estimate the unknown function $g(X)$ we will use Regression Trees and Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimesions:  51759 212"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>LOGVALUE</th><th scope=col>LOT</th><th scope=col>UNITSF</th><th scope=col>CLIMB</th><th scope=col>DIRAC</th><th scope=col>NUMAIR</th><th scope=col>BUSPER</th><th scope=col>EXCLUS</th><th scope=col>HOWH</th><th scope=col>NUMCOLD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.6931472</td><td>   900   </td><td>2314.18  </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td> 6       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>0.6931472</td><td> 14520   </td><td>2500.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td>10       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>0.6931472</td><td> 11000   </td><td>7507.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.000000 </td><td>0        </td><td>0        </td><td> 9       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.6094379</td><td> 23760   </td><td> 830.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.000000 </td><td>0        </td><td>0        </td><td>10       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.6094379</td><td>  3000   </td><td>1100.00  </td><td>2.011952 </td><td>1.51928  </td><td>4.000000 </td><td>0        </td><td>0        </td><td> 7       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.6094379</td><td> 44000   </td><td>1000.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td> 6       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.6094379</td><td>220000   </td><td>2000.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.000000 </td><td>1        </td><td>0        </td><td> 8       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.6094379</td><td>220000   </td><td>1800.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td>10       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.6094379</td><td> 33000   </td><td>1200.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>1        </td><td>0        </td><td>10       </td><td>1.828194 </td></tr>\n",
       "\t<tr><td>1.9459101</td><td> 44000   </td><td>1957.00  </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td> 8       </td><td>1.828194 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " LOGVALUE & LOT & UNITSF & CLIMB & DIRAC & NUMAIR & BUSPER & EXCLUS & HOWH & NUMCOLD\\\\\n",
       "\\hline\n",
       "\t 0.6931472 &    900    & 2314.18   & 2.011952  & 1.51928   & 1.730885  & 0         & 0         &  6        & 1.828194 \\\\\n",
       "\t 0.6931472 &  14520    & 2500.00   & 2.011952  & 1.51928   & 1.730885  & 0         & 0         & 10        & 1.828194 \\\\\n",
       "\t 0.6931472 &  11000    & 7507.00   & 2.011952  & 1.51928   & 1.000000  & 0         & 0         &  9        & 1.828194 \\\\\n",
       "\t 1.6094379 &  23760    &  830.00   & 2.011952  & 1.51928   & 1.000000  & 0         & 0         & 10        & 1.828194 \\\\\n",
       "\t 1.6094379 &   3000    & 1100.00   & 2.011952  & 1.51928   & 4.000000  & 0         & 0         &  7        & 1.828194 \\\\\n",
       "\t 1.6094379 &  44000    & 1000.00   & 2.011952  & 1.51928   & 1.730885  & 0         & 0         &  6        & 1.828194 \\\\\n",
       "\t 1.6094379 & 220000    & 2000.00   & 2.011952  & 1.51928   & 1.000000  & 1         & 0         &  8        & 1.828194 \\\\\n",
       "\t 1.6094379 & 220000    & 1800.00   & 2.011952  & 1.51928   & 1.730885  & 0         & 0         & 10        & 1.828194 \\\\\n",
       "\t 1.6094379 &  33000    & 1200.00   & 2.011952  & 1.51928   & 1.730885  & 1         & 0         & 10        & 1.828194 \\\\\n",
       "\t 1.9459101 &  44000    & 1957.00   & 2.011952  & 1.51928   & 1.730885  & 0         & 0         &  8        & 1.828194 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| LOGVALUE | LOT | UNITSF | CLIMB | DIRAC | NUMAIR | BUSPER | EXCLUS | HOWH | NUMCOLD |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.6931472 |    900    | 2314.18   | 2.011952  | 1.51928   | 1.730885  | 0         | 0         |  6        | 1.828194  |\n",
       "| 0.6931472 |  14520    | 2500.00   | 2.011952  | 1.51928   | 1.730885  | 0         | 0         | 10        | 1.828194  |\n",
       "| 0.6931472 |  11000    | 7507.00   | 2.011952  | 1.51928   | 1.000000  | 0         | 0         |  9        | 1.828194  |\n",
       "| 1.6094379 |  23760    |  830.00   | 2.011952  | 1.51928   | 1.000000  | 0         | 0         | 10        | 1.828194  |\n",
       "| 1.6094379 |   3000    | 1100.00   | 2.011952  | 1.51928   | 4.000000  | 0         | 0         |  7        | 1.828194  |\n",
       "| 1.6094379 |  44000    | 1000.00   | 2.011952  | 1.51928   | 1.730885  | 0         | 0         |  6        | 1.828194  |\n",
       "| 1.6094379 | 220000    | 2000.00   | 2.011952  | 1.51928   | 1.000000  | 1         | 0         |  8        | 1.828194  |\n",
       "| 1.6094379 | 220000    | 1800.00   | 2.011952  | 1.51928   | 1.730885  | 0         | 0         | 10        | 1.828194  |\n",
       "| 1.6094379 |  33000    | 1200.00   | 2.011952  | 1.51928   | 1.730885  | 1         | 0         | 10        | 1.828194  |\n",
       "| 1.9459101 |  44000    | 1957.00   | 2.011952  | 1.51928   | 1.730885  | 0         | 0         |  8        | 1.828194  |\n",
       "\n"
      ],
      "text/plain": [
       "   LOGVALUE  LOT    UNITSF  CLIMB    DIRAC   NUMAIR   BUSPER EXCLUS HOWH\n",
       "1  0.6931472    900 2314.18 2.011952 1.51928 1.730885 0      0       6  \n",
       "2  0.6931472  14520 2500.00 2.011952 1.51928 1.730885 0      0      10  \n",
       "3  0.6931472  11000 7507.00 2.011952 1.51928 1.000000 0      0       9  \n",
       "4  1.6094379  23760  830.00 2.011952 1.51928 1.000000 0      0      10  \n",
       "5  1.6094379   3000 1100.00 2.011952 1.51928 4.000000 0      0       7  \n",
       "6  1.6094379  44000 1000.00 2.011952 1.51928 1.730885 0      0       6  \n",
       "7  1.6094379 220000 2000.00 2.011952 1.51928 1.000000 1      0       8  \n",
       "8  1.6094379 220000 1800.00 2.011952 1.51928 1.730885 0      0      10  \n",
       "9  1.6094379  33000 1200.00 2.011952 1.51928 1.730885 1      0      10  \n",
       "10 1.9459101  44000 1957.00 2.011952 1.51928 1.730885 0      0       8  \n",
       "   NUMCOLD \n",
       "1  1.828194\n",
       "2  1.828194\n",
       "3  1.828194\n",
       "4  1.828194\n",
       "5  1.828194\n",
       "6  1.828194\n",
       "7  1.828194\n",
       "8  1.828194\n",
       "9  1.828194\n",
       "10 1.828194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Housing = read.csv(unzip('ahs_clean.zip'))   \n",
    "# Reads csv files into R\n",
    "\n",
    "cat('Dataset dimesions: ',dim(Housing))                  \n",
    "# Returns the dimension of the dataset             \n",
    "\n",
    "Housing$LOGSQFT = log(Housing$UNITSF)\n",
    "Housing$LOGLOT = log(Housing$LOT)\n",
    "\n",
    "Housing[1:10,1:10]                                         \n",
    "# Examine the first few rows and columns of our data                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already examined this data, let's go ahead and use the larger model with 60 covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "\n",
    "data_split = function(data,split=.1){\n",
    "\n",
    "    N = dim(data)[1]                         \n",
    "    random = sample(1:N,N,replace=F)          \n",
    "    # Randomly choose the index for the splits\n",
    "    data = data[random,]                      \n",
    "    # Shuffle data\n",
    "    test = data[1:round(N*split),]            \n",
    "    # Splits\n",
    "    train = data[(round(N*split)+1):N,]\n",
    "\n",
    "    \n",
    "    return(list(train,test))\n",
    "}\n",
    "\n",
    "\n",
    "tmp = data_split(Housing) \n",
    "train_test = tmp[[1]]\n",
    "validation = tmp[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train1 = train_test[c('LOGVALUE','LOGLOT','LOGSQFT','BEDRMS','BATHS',\n",
    "                      'REGION','METRO','KITCHEN','DISH','WASH','DRY',\n",
    "                      'COOK','DENS','DINING','FAMRM','HALFB','LIVING',\n",
    "                      'OTHFN','RECRM','PORCH','SINK','LAUNDY','FLOORS',\n",
    "                      'CONDO','ROOMS','PLUMB','NOWIRE','AGE')]\n",
    "\n",
    "train1 = cbind(train1,select(train_test,starts_with('Roach')),\n",
    "               select(train_test,starts_with('Rats')),\n",
    "              select(train_test,starts_with('Water')),\n",
    "               select(train_test,starts_with('Cellar')),\n",
    "               select(train_test,starts_with('Num')),\n",
    "               select(train_test,starts_with('Freeze')))\n",
    "\n",
    "\n",
    "val1 = validation[colnames(train1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "How do we actually estimate a decision tree? Well if we are predicting the mean, then we are using a MSE loss function. For regression trees will predict: \n",
    "\n",
    "$\\hat{c}_m = \\frac{\\sum_i y_i 1(x\\in R_m)}{\\sum_i 1(x\\in R_m)}$\n",
    "\n",
    "This is nothing more than the average outcome given the covariate values in the region.\n",
    "\n",
    "\n",
    "When fitting a regression tree one needs to decide how to generate the splits that make up the tree. Further one needs to decide how deep to make the tree. There are different data-driven approaches to making these selections. In this module we will choose the model based on the complexity:\n",
    "\n",
    "$C_\\alpha(T) = \\sum_m N_m Q_m(T) + \\alpha |T|$\n",
    "\n",
    "$N_m$ is the number of observations in node $m$\n",
    "\n",
    "$|T|$ is the number of terminal nodes in tree T\n",
    "\n",
    "$Q_m(T)=\\frac{1}{N_m} \\sum_{i:x_i\\in R_m} (y_i-\\hat{c}_m)^2$\n",
    "\n",
    "## Trees in R\n",
    "\n",
    "Instead of manually choosing the depth of our tree, we will use 'Carat' to tune the penalty $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cp</th><th scope=col>RMSE</th><th scope=col>Rsquared</th><th scope=col>MAE</th><th scope=col>RMSESD</th><th scope=col>RsquaredSD</th><th scope=col>MAESD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.03478663 </td><td>0.8988914  </td><td>0.2029682  </td><td>0.5972884  </td><td>0.02009004 </td><td>0.019166297</td><td>0.007800018</td></tr>\n",
       "\t<tr><td>0.07372741 </td><td>0.9345346  </td><td>0.1381689  </td><td>0.6275925  </td><td>0.02999106 </td><td>0.042824540</td><td>0.023319109</td></tr>\n",
       "\t<tr><td>0.10976974 </td><td>0.9734860  </td><td>0.1069229  </td><td>0.6667220  </td><td>0.02987317 </td><td>0.001567048</td><td>0.031349563</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " cp & RMSE & Rsquared & MAE & RMSESD & RsquaredSD & MAESD\\\\\n",
       "\\hline\n",
       "\t 0.03478663  & 0.8988914   & 0.2029682   & 0.5972884   & 0.02009004  & 0.019166297 & 0.007800018\\\\\n",
       "\t 0.07372741  & 0.9345346   & 0.1381689   & 0.6275925   & 0.02999106  & 0.042824540 & 0.023319109\\\\\n",
       "\t 0.10976974  & 0.9734860   & 0.1069229   & 0.6667220   & 0.02987317  & 0.001567048 & 0.031349563\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| cp | RMSE | Rsquared | MAE | RMSESD | RsquaredSD | MAESD |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 0.03478663  | 0.8988914   | 0.2029682   | 0.5972884   | 0.02009004  | 0.019166297 | 0.007800018 |\n",
       "| 0.07372741  | 0.9345346   | 0.1381689   | 0.6275925   | 0.02999106  | 0.042824540 | 0.023319109 |\n",
       "| 0.10976974  | 0.9734860   | 0.1069229   | 0.6667220   | 0.02987317  | 0.001567048 | 0.031349563 |\n",
       "\n"
      ],
      "text/plain": [
       "  cp         RMSE      Rsquared  MAE       RMSESD     RsquaredSD  MAESD      \n",
       "1 0.03478663 0.8988914 0.2029682 0.5972884 0.02009004 0.019166297 0.007800018\n",
       "2 0.07372741 0.9345346 0.1381689 0.6275925 0.02999106 0.042824540 0.023319109\n",
       "3 0.10976974 0.9734860 0.1069229 0.6667220 0.02987317 0.001567048 0.031349563"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- train(\n",
    "  LOGVALUE ~.,data = train1,               \n",
    "    # formula for the model\n",
    "  method = \"rpart\",                           \n",
    "    # Regression Trees\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", \n",
    "    number = 5,             \n",
    "      # 5 fold cross-validation\n",
    "  )\n",
    ")\n",
    "\n",
    "model$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "According to 5-fold cross-validation the best complexity parameter is $\\approx 0.03$. We only tried three different complexity parameters, but this value appears to have the best performance across all three metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.matrix.default(attr(frame, \"terms\"), frame):\n",
      "\"the response appeared on the right-hand side and was dropped\"Warning message in model.matrix.default(attr(frame, \"terms\"), frame):\n",
      "\"problem with term 1 in model.matrix: no columns are assigned\"Warning message in cats * !isord:\n",
      "\"longer object length is not a multiple of shorter object length\""
     ]
    }
   ],
   "source": [
    "formula = as.formula(paste(\"LOGVALUE~\",\n",
    "            paste(colnames(train1)[1:dim(train1)[2]],collapse=\"+\")))\n",
    "\n",
    "fit_tree = rpart(formula,data=train1, control=c(cp=model$bestTune))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Tree\n",
    "\n",
    "A great feature of regression trees is visualization and interpretation are very straightforward. We can visualize and interpret the regression tree with the following plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABwlBMVEUAAAAgNEEoOUIrR1cw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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpart.plot(fit_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see predicted (rounded) values for this tree above the frequency of those predictions. The interpretation is very easy as well. Consider a value of $REGION$ less than 3.5. The tree tells us that if this is the case we move to the left side. Then we see if the $LOGSQFT$ is less than 7.7. If it is we move left, if not we move right. This process continuous for each node in the tree until the final nodes in the tree. \n",
    "\n",
    "## Performance\n",
    "\n",
    "Let's see how we do on the validation set with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree RMSE:  1.016211"
     ]
    }
   ],
   "source": [
    "yhat = predict(fit_tree,val1)\n",
    "rmse = sqrt(mean((val1$LOGVALUE-yhat)^2))\n",
    "cat('\\nTree RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are great for interpretation, but not the best for actual prediction. The RMSE we got is worse than the linear models we estimated in the previous module.\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "One idea that is common in machine learning is to combine weak learners together and produce an ensemble. When one does this with trees the method is known as random forests. The basic idea is combine many shallow trees with only a subset of the covariates and average the predictions. As usual with these algorithms, there are many hyperparameters to choose. To get us started we will fix the number of trees we use to $10$. This is very small for a random forest, but we are just testing this out right now. A typically number of trees to use in practice would be around $200$. Another tuning parameter we need to determine is the number of covariates to split on. Remember that random forests reduce the correlation between trees by subsetting the covariates in each tree. Let's use cross-validation to determine this number over the choices $5,20$, and $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.matrix.default(Terms, m, contrasts):\n",
      "\"the response appeared on the right-hand side and was dropped\"Warning message in model.matrix.default(Terms, m, contrasts):\n",
      "\"problem with term 1 in model.matrix: no columns are assigned\""
     ]
    }
   ],
   "source": [
    "# Grid for number of covariates\n",
    "rf_grid = expand.grid(mtry = c(5, 20, 50), \n",
    "          splitrule = c(\"variance\"), min.node.size=c(5))\n",
    "\n",
    "# train and evaluate the random forest with 5-fold CV\n",
    "fit_forest = train(formula,\n",
    "                   data = train1, \n",
    "                   method = 'ranger',\n",
    "                   trControl = \n",
    "                   trainControl(method = 'cv',\n",
    "                                number = 5),\n",
    "                   tuneGrid = rf_grid, \n",
    "                   num.trees=10\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>mtry</th><th scope=col>splitrule</th><th scope=col>min.node.size</th><th scope=col>RMSE</th><th scope=col>Rsquared</th><th scope=col>MAE</th><th scope=col>RMSESD</th><th scope=col>RsquaredSD</th><th scope=col>MAESD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 5         </td><td>variance   </td><td>5          </td><td>0.7605869  </td><td>0.4310417  </td><td>0.4879596  </td><td>0.01776097 </td><td>0.01157978 </td><td>0.005734571</td></tr>\n",
       "\t<tr><td>20         </td><td>variance   </td><td>5          </td><td>0.7585294  </td><td>0.4349509  </td><td>0.4841728  </td><td>0.01592480 </td><td>0.01488887 </td><td>0.006118859</td></tr>\n",
       "\t<tr><td>50         </td><td>variance   </td><td>5          </td><td>0.7710808  </td><td>0.4224557  </td><td>0.4925165  </td><td>0.01596987 </td><td>0.01787877 </td><td>0.005624229</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " mtry & splitrule & min.node.size & RMSE & Rsquared & MAE & RMSESD & RsquaredSD & MAESD\\\\\n",
       "\\hline\n",
       "\t  5          & variance    & 5           & 0.7605869   & 0.4310417   & 0.4879596   & 0.01776097  & 0.01157978  & 0.005734571\\\\\n",
       "\t 20          & variance    & 5           & 0.7585294   & 0.4349509   & 0.4841728   & 0.01592480  & 0.01488887  & 0.006118859\\\\\n",
       "\t 50          & variance    & 5           & 0.7710808   & 0.4224557   & 0.4925165   & 0.01596987  & 0.01787877  & 0.005624229\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| mtry | splitrule | min.node.size | RMSE | Rsquared | MAE | RMSESD | RsquaredSD | MAESD |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "|  5          | variance    | 5           | 0.7605869   | 0.4310417   | 0.4879596   | 0.01776097  | 0.01157978  | 0.005734571 |\n",
       "| 20          | variance    | 5           | 0.7585294   | 0.4349509   | 0.4841728   | 0.01592480  | 0.01488887  | 0.006118859 |\n",
       "| 50          | variance    | 5           | 0.7710808   | 0.4224557   | 0.4925165   | 0.01596987  | 0.01787877  | 0.005624229 |\n",
       "\n"
      ],
      "text/plain": [
       "  mtry splitrule min.node.size RMSE      Rsquared  MAE       RMSESD    \n",
       "1  5   variance  5             0.7605869 0.4310417 0.4879596 0.01776097\n",
       "2 20   variance  5             0.7585294 0.4349509 0.4841728 0.01592480\n",
       "3 50   variance  5             0.7710808 0.4224557 0.4925165 0.01596987\n",
       "  RsquaredSD MAESD      \n",
       "1 0.01157978 0.005734571\n",
       "2 0.01488887 0.006118859\n",
       "3 0.01787877 0.005624229"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_forest$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Random Forests\n",
    "\n",
    "First of all we notice that we have a big reduction in RMSE when we use random forests over trees, regardless of the tuning parameter. Secondly the performance is similar across the three specifications although we would expect the RMSE to be best when the number of covariates used is not too small or too large. Why is this? Remember that when the number of covariates used approaches the total number then we are simply bagging trees. These trees will be too highly correlated and prediction will suffer. On the other hand if we use too few covariates the trees will not be able to learn interesting patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Module 5\n",
    "\n",
    "In this module we have seen how to build a regression tree. In addition to estimating the model we saw how easy it was to build a graph based on the tree. This graph allows for a straightforward interpretation of the model. Unfortunately, while the interpretation was very intuitive, the model performance was not very good. To fix this problem we introduced random forests and explained a few of the tuning parameters that need to be adjusted. Let's see how the results of the previous section change when we allow the number of trees to grow.\n",
    "\n",
    "1. Use the code from the previous section to cross-validate the number of covariates to split on\n",
    "2. Do this for 100 and 200 Trees\n",
    "4. Do the results become more clear?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
